%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2013 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2013,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
% \usepackage{subfigure}
\usepackage{subcaption}
\usepackage{multicol}

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% For math
\usepackage{amsmath}
\usepackage{siunitx}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2013} with
% \usepackage[nohyperref]{icml2013} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2013}
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
% \usepackage[accepted]{icml2013}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{6.867: Homework 3}

\begin{document}

\twocolumn[
  \icmltitle{6.867: Homework 3}

  % % It is OKAY to include author information, even for blind
  % % submissions: the style file will automatically remove it for you
  % % unless you've provided the [accepted] option to the icml2013
  % % package.
  % \icmlauthor{Your Name}{email@yourdomain.edu}
  % \icmladdress{Your Fantastic Institute,
  %             314159 Pi St., Palo Alto, CA 94306 USA}
  % \icmlauthor{Your CoAuthor's Name}{email@coauthordomain.edu}
  % \icmladdress{Their Fantastic Institute,
  %             27182 Exp St., Toronto, ON M6H 2T1 CANADA}

  % You may provide any keywords that you
  % find helpful for describing your paper; these are used to populate
  % the "keywords" metadata in the PDF but will not be shown in the document
  \icmlkeywords{boring formatting information, machine learning, ICML}

  \vskip 0.3in
]

\section{Neural Networks}
In this section, we explore neural networks and various choices for the number of hidden layers, the number of neurons per hidden layer, regularization, binary classification, and multiclass classification. We investigate how our choice of these variables impacts the overall classification rates and how these decisions are incorporated into our implementation of neural networks.

\subsection{ReLU + Softmax}
We have here implemented a neural network in which all the hidden units have a ReLu activation function and the final output layer has a Softmax activation function. This is incorporated into the implementation where in the output layer, the activation $\alpha_i$ for class $i$ is determined by the Softmax function:
$$f(z)_i = \frac{e^{z_i}}{\sum_{j=1}^k e^{z_j}}$$
Thus, for a $k$-class classification problem, the output layer has $k$ neurons each with Softmax activation. The prediction is therefore given by the argmax of the $alpha$ values calculated by the Softmax for each of the $k$ neurons. \\

The cross entropy loss is the loss function used here and is given by:
$$-\sum_{i=1}^k y_i \log(f(z)_i)$$
This formulation of the loss function is essentially a calculation of our confidence in the correct classification. In other words, the loss is the log likelihood of the correctly classified class by the weights assigned in our training of the neural network model. This loss is incorporated into the implementation via the output error. This output error is then involved in backpropagation to determine $\delta$ values for each layer in the neural network, which is subsequently used to calculate the gradient used for stochastic gradient descent. Thus, our choice of the Softmax function as the output activation function and the cross entropy loss as the loss function in this case is critically important in our overall implementation. Different choices of these functions could fundamentally change the training and thus classification accuracy of our overall model.

\subsection{Initialization}
We can initialize the weights to be randomly selected from a Gaussian distribution with zero mean and standard deviation $\frac{1}{\sqrt{m}}$ for an $m$x$n$ weight matrix. Choosing to initialize weights to these values is a reasonable decision because $m$ represents the number of input neurons in each of our layers. This is selected to control the variance of the distribution in order to ensure that each layer in the neural network is agnostic to the number of inputs it receives, and instead depends on the values of the inputs.

\subsection{Regularization}
This would impact the pseudocode because now we must also incorporate this additional regularization term when taking the gradient of the loss function. Since this gradient is used in the stochastic gradient descent update it ultimately impacts the entire algorithm. More specifically, the update step before regularization was:
$$\theta \leftarrow \theta - \eta \frac{\delta l(y,F(x;\theta))}{\delta \theta}$$
With regularization, the update step now becomes:

$$\theta \leftarrow \theta - \eta \frac{\delta J}{\delta \theta}$$
where
$$J(\theta) = L(\theta) + R(\theta)$$
and
$$R(\theta) = \lambda *(\sum_{ij} {w_{ij}^{(1)}}^2 + \sum_{ij} {w_{ij}^{(2)}}^2)$$
Furthermore,
$$J'(\theta) = L'(\theta) + R'(\theta)$$
$L'(\theta)$ has already been calculated and $R'(\theta)$ is given by:
$$R'(\theta) = 2* \lambda * W$$


\subsection{Binary classification}
We tested our implementation on the 2D datasets provided in HW2. The accuracies for the training and testing data sets are presented for different architectures of the neural network in Tables 1 and 2. Interestingly, we found that our implementation of the neural network exhibited significant variability and instability, even for small networks. This could be attributed to the randomly initialized weights, as well as the fact that the algorithm terminates when the validation error no longer decreases. Other choices of the termination criterion would have affected the overall accuracy of the algorithm however. The numbers presented in Tables 1 and 2 therefore illustrate the average accuracies when each neural net training process was run 20 times with a learning rate of $0.1$.

\begin{table}
  \begin{center}
    \begin{tabular}{ | c | c | c | c | c | }
      \hline
          & (5) & (100) & (5,5) & (100,100) \\ \hline
      1   & 0.995      & 0.997        & 0.945      & 0.997        \\ \hline
      2   & 0.819      & 0.835        & 0.785      & 0.840        \\ \hline
      3   & 0.965      & 0.961        & 0.948      & 0.968        \\ \hline
      4   & 0.934      & 0.951        & 0.919      & 0.951        \\ \hline
    \end{tabular}
  \end{center}
  \caption{Training accuracy, $l$ layers, $n$ neurons/layer}
\end{table}

\begin{table}
  \begin{center}
    \begin{tabular}{ | c | c | c | c | c | }
      \hline
          & (5)   & (100) & (5,5) & (100,100) \\ \hline
      1   & 0.993      & 0.997        & 0.945      & 0.995        \\ \hline
      2   & 0.799      & 0.813        & 0.768      & 0.823        \\ \hline
      3   & 0.961      & 0.962        & 0.945      & 0.948        \\ \hline
      4   & 0.942      & 0.956        & 0.925      & 0.958        \\ \hline
    \end{tabular}
  \end{center}
  \caption{Testing accuracy, $l$ layers, $n$ neurons/layer}
\end{table}

In general we found that the training and testing accuracies were fairly comparable with each other. Furthermore, the number of neurons and the number of layers did not significantly affect the accuracy rates, although the classification rate did suffer slightly in a smaller neural net (1 layer with 5 neuron per layer and 2 layers with 5 neurons per layer). The neural network consistently had the worst performance for dataset 2, which makes intuitive sense as dataset 2 was not linearly separable and had many overlapping samples of different classes.

We additionally found that learning rate made a significant difference on the performance of the algorithm. This is plotted in Figure 1, in which the learning rates for different neural net architectures is compared with the average (over 20 runs as before) testing classification accuracy. It is interesting to note that the general shape of the curve plotting learning rate against testing accuracy is generally the same even for different architectures of the neural network. More specifically, accuracy peaks at a learning rate of about 0.1 to 1, and suffers quite significantly for greater and smaller learning rates. Furthermore, for suboptimal choices of the learning rate, the accuracy sometimes dropped below 50\%, at which point, the classifier effectively performs no better than random guess. In this case, these results were generated based on dataset 1, but similar results are fairly likely for other datasets.

\begin{figure}[width=\linewidth]
\centering
\begin{multicols}{2}
  \includegraphics[width=1.2\linewidth]{code/P1/learning_rate_vs_accuracy,(5,5).png}
  \includegraphics[width=1.2\linewidth]{code/P1/learning_rate_vs_accuracy,(100).png}
\end{multicols}
\caption{Learning rate vs. testing accuracy}
\end{figure}

Comparing these results with the accuracies from HW2, we see that the neural network performed significantly better than logistic regression with various types of regularization for non-linearly separable data. For linearly separable data, the accuracies were very comparable. Comparing these results with support vector machines, the neural network performed approximately as well as an SVM with a linear kernel for all the datasets with the exception of dataset 4, which displayed very poor performance. For SVMs with a radial basis kernel however, we were able to improve on the performance of a linear kernel SVM for dataset 4, and overall, the radial basis SVM displayed comparable, if not slightly improved performance, over neural nets. This comparative analysis is illustrated in Table 3, where the neural net accuracies are generated based on a neural net of one layer with 100 neurons per layer.

\begin{table}
  \begin{center}
    \begin{tabular}{ | c | c | c | c | c | }
      \hline
          & Logistic   & Linear SVM   & RBF SVM    & Neural net \\ \hline
      1   & 0.99       & 0.99         & 0.99       & 0.997        \\ \hline
      2   & 0.805      & 0.81         & 0.835      & 0.813        \\ \hline
      3   & 0.97       & 0.97         & 0.96       & 0.962        \\ \hline
      4   & 0.50       & 0.508        & 0.963      & 0.956        \\ \hline
    \end{tabular}
  \end{center}
  \caption{Testing accuracies of different algorithms}
\end{table}

\subsection{Multi-class classification}
We ran our neural network on the multi-class classification problem of identifying digits from the MNIST dataset. In our analysis, we used the same training, validation, and testing dataset sizes as suggested in HW2 for consistency. In this multi-class classification problem, we found that performance in identifying numbers was quite surprisingly low (at a little better than random chance). Nonetheless, we compared the testing accuracy against different numbers of neurons per layer, different numbers of hidden layers, and different learning rates for different neural network architectures.

Figure 2 illustrates the testing accuracy rate as a function of the number of neurons per layer in a one- and two-hidden layer neural network. It is interesting to note that in this case, accuracy increases as the number of neurons increases, which makes intuitive sense as an increased number of neurons can more effectively discover new features. Interestingly however, we do not observe a fall in the performance of the neural network at extremely large values of $n$, the number of neurons per layer (e.g. 1000), as one might expect could result due to overfitting. Here we simply see the performance of the network improve from a little better than random chance when $n=10$ to about 22\% when $n=1000$. The performance results for a one-layer neural network versus a two-layer neural network were found to be very comparable. Note that the accuracies calculated here are again an average of 20 runs of training.

\begin{figure}[width=\linewidth]
\centering
\begin{multicols}{2}
  \includegraphics[width=1.2\linewidth]{code/P1/accuracy_vs_neurons,mnist.png}
  \includegraphics[width=1.2\linewidth]{code/P1/accuracy_vs_neurons,mnist,l2.png}
\end{multicols}
\caption{Number of neurons vs. testing accuracy}
\end{figure}

We additionally explored the testing accuracy as a function of the number of layers in the neural network. These results are presented in Figure 3, in which the neural network had 5 neurons and 100 neurons per hidden layer. Both sizes of neural networks had similar trends, in that performance generally peaked at around 2 or 3 hidden layers. Performance generally deteriorated for smaller or larger values of $l$, which is in support of our hypothesis that extremely complex neural nets might overfit to the training data and thus suffer during testing on unseen data samples. Furthermore, as is consistent with our hypothesis, we can see that peak performance for the neural network with five neurons per layer performed worse (at around 12\%) than the neural network with 100 neurons per layer (at around 18\%).

\begin{figure}[width=\linewidth]
\centering
\begin{multicols}{2}
  \includegraphics[width=1.2\linewidth]{code/P1/accuracy_vs_layers,mnist,n5.png}
  \includegraphics[width=1.2\linewidth]{code/P1/accuracy_vs_layers,mnist,n100.png}
\end{multicols}
\caption{Number of layers vs. testing accuracy}
\end{figure}

Figure 4 illustrates the relationship between the learning rate and the testing accuracy for neural networks of size $n=5,l=2$ and $n=100,l=1$. In this situation, we again found that there was an optimal learning rate, below and above which the performance of the neural network significantly suffered. Interestingly however, we found that the optimal learning rate for a neural network with $n=5, l=2$ was generally much smaller for the multi-class classification problem with MNIST ($\eta^*$ = 1\textsc{e}-5) than with the binary classification problem ($\eta^*$ = 1\textsc{e}-2). We did not observe this discrepancy in behavior comparing a neural network with $n=100, l=1$ in the multi-class classification problem versus in the binary classification problem ($\eta^*$ = 1\textsc{e}-2 for both).

\begin{figure}[width=\linewidth]
\centering
\begin{multicols}{2}
  \includegraphics[width=1.2\linewidth]{code/P1/accuracy_vs_learning_rate,mnist,(5,5).png}
  \includegraphics[width=1.2\linewidth]{code/P1/accuracy_vs_learning_rate,mnist,(100).png}
\end{multicols}
\caption{Learning rate vs. testing accuracy}
\end{figure}

Note that in this case, normalization of the data is especially important. This is because of the activation function we have chosen to use in each of the neurons of the hidden layers. More specifically, the ReLu activation function takes the maximum of 0 and $z$. For unnormalized data however, $z > 0$, and this activation function would not adequately fit the architecture of the neural network. Normalization is therefore essential in order to have the data input fit the architecture demanded by a neural network.

Overall we found that for the multi-class classification problem explored here, a neural network with 3 hidden layers and a large number of neurons per hidden layer (e.g. 1000) with an optimized learning rate performs best. We found that under this architecture, the optimal learning rate was $\eta^*=$1\textsc{E}-2 with a final testing performance of 39\% (Figure 4).

\begin{figure}[width=\linewidth]
\centering
  \includegraphics[width=0.8\linewidth]{code/P1/accuracy_vs_learning_rate,mnist,(1000,1000,1000).png}
\caption{Best neural network architecture}
\end{figure}


\section{Convolutional Neural Networks}
In this section, we explore the application of convolutional neural networks in performing image classification.

\subsection{Convolutional filter receptive field}
The dimensions of the receptive field for a node in $Z_2$ is $7 x 7$. With this in mind, we see that it is effective to build convolutional networks deeper in order to increase the receptive fields for subsequent nodes. This is effective because it allows the network to learn additional features that take into account larger and larger amounts of the original image's pixels.

\subsection{Run the Tensorflow conv net}
In the "define\_tensorflor\_graph" graph function we see that there could be a maximum of 6 layers and a minimum of 4 layers. Only 2 of the layers are convolutional. 2 other ones are regular hidden ones and the 2 optional ones are pooling layers. The relu activation function is used on the hidden nodes. The softmax loss function with cross entropy is being used to train the network. The loss is being minimized with gradient descent. When the convolutional network is run, we get a batch training accuracy of 100\% and a validation accuracy of 63.2\%. What this means is that the network is classifying extremely well on the training data, while not so well on the validation datasets, which points towards overfitting.

\subsection{Add pooling layers}
By adding pooling layers to our convolutional network, our results definitely changed. Specifically, we used a layer 1 pool filter size of 5, a layer 1 pool stride of 2, a layer 2 pool filter size of 2, and a layer 2 pool stride size of 2. After the 1500th step, this achieved a batch training accuracy of 100\% and a validation accuracy of 69\%. Interestingly enough, after the 1400th step, the training accuracy was 70\% and the validation accuracy was also 70\%. With these results it appears that max pooling marginally reduces the training accuracy while improving the validation accuracy. In a sense, it almost serves as a form of regularization that is able to prevent overfitting because now instead of the final layer processing information from all the original pixels the pooling allows clusters of pixels to be expressed as one value which enables better generalization.

\subsection{Regularize your network!}
\subsubsection{Dropout}
Dropout is a form of regularization that involves getting rid of certain activations. This prevents the units from co-adapting too much and is a good strategy to prevent overfitting. Through testing a variety of dropout values ranging from 0.5 to 1.0, it was discovered that optimum dropout rates for this convolutional network were in the range from 0.8 to 0.9. For a dropout rate of 0.9, we were able to get 70\% training accuracy and 71.3\% validation accuracy. Once the dropout rate hit 0.7, the results were noticeably worse (60\% training accuracy and 59.6\% validation accuracy). The one interesting thing we did notice is that the lower the dropout rate the less time it takes for the algorithm to complete. This makes intuitive sense because it means less calculations need to be made.

\subsubsection{Weight regularization}
Another way to avoid overfitting is to simply add an additional term to the loss function that penalizes the norm of the network's weights. This is a technique that we've used countless times in the class already. Through testing a variety of weight penalties from 0.0 to 1.0, it was discovered that the optimum weight penalty is between 0.01 and 0.05. With a penalty of 0.02, the training accuracy was 70\% while the validation accuracy was 74.7\%. Any penalty greater than 0.1 started causing poor performance. For example, a penalty of 0.2 resulted in a training accuracy of 60\% and a validation accuracy of 49.4\%. 

\subsubsection{Data augmentation}
Another effective regularization technique is to augment the data with transformed versions of the original samples. When we use an augmented dataset that contains randomly flipped and distorted versions of the original training images as well as increase the number of training steps to 6001, we were able to get a batch training accuracy of 90\% and a validation accuracy of 71.3\%. This definitely is better than the original validation accuracy we got, which leads me to believe that it did in part solve the overfitting problem. 

\subsubsection{Early stopping}
The final regularization technique that we tested was early stopping. The technique is quite simply and simply involves stopping the training as the validation accuracy starts to plateau or decrease. Through many tests, it was found that the validation accuracy starts to plateau at 1200 steps. At this time, the training accuracy is 70\% and the validation accuracy is 70.1\%. 

\subsubsection{Discussion}
While all regularization techniques were moderately effective, it seems that from rigorous testing that weight regularization was just slightly better than the rest by a couple percentage points.

\subsection{Experiment with your architecture}
In this section we will investigate other means to tweaking our architecture in order to improve the performance of the convolutional network. First we will look at what happens when we increase the filter size, stride, and depth of the convolutional layers. 

\textbf{\textit{Filter size:}}
When the filter size is increased it is clear that there are two predominant effects. First, the total algorithm run time increases. When the filter size is 5 for both layers, the algorithm finishes in 21.78 seconds while when the filter size is 15 for both layers, the algorithm finishes in 96.24 seconds. The second effect that I noticed is that the validation error plateaus much earlier. For filter sizes of 5 and 7, the validation error plateaus at around 1200 steps while for filter sizes of 10 and 15, the validation error plateaus at around 900 steps. The best validation error was achieved with a filter size of 10, which was able to get a training accuracy of 100\% and a validation accuracy of 75.9\%.

\textbf{\textit{Stride:}}
Increasing the stride seems to have the exact opposite impacts compared with increasing filter size. Specifically, for a given max number of training steps, the higher the stride, the faster the algorithm takes to complete. For example, with 3001 steps, it takes the algorithm 48.92 seconds to complete when the stride is 2 while only 23.09 seconds to complete when the stride is 4. This makes intuitive sense because with a larger stride, there are less patches to deal with. The other discovery we made regarding stride is that with a higher stride it take the algorithm many more steps before the validation accuracy plateaus. With a stride of 2, the validation error plateaus at around 1200 steps while with a stride of 4, the validation error doesn't plateau until step 2800.

\textbf{\textit{Depth:}}
Increasing the depth seems to yield effects very similar to increasing filter size. First off, as depth increases, it takes the algorithm a longer time to run to completion. For example, with total training steps set to 1501, it took the algorithm 26.31 seconds to finish when the depth was 16 and 64.76 seconds to finish when the depth was 64. Additionally, it was observed that increasing the depth made the validation error plateau faster. At a depth of 16, the validation error plateaued at 1400 steps, at a depth of 32, the validation error plateaued at 1100 steps, and at a depth of 64, the validation error plateaued at 500 steps. 

After we investigated the impact of each of the above factors independently, we then proceeded to test combinations of the factors together as architectures. We tested a pyramidal shaped architecture with the feature maps gradually decreasing in height and width while increasing in depth, a flat architecture, and an architecture with an inverse shape. Here's what we found:


\textbf{\textit{Pyramid-shaped:}}
The pyramid-shaped architecture performed relatively well. It completed 1500 steps in 54.53 seconds while converging (validation error plateaus) at the 1100 step mark to yield a training accuracy of 100\% and a validation accuracy of 71.3\%.


\textbf{\textit{Flat:}}
The flat architecture did slightly better compared with the pyramid-shaped one. It completed 1500 steps in 52.36 seconds while converging at the 900 step mark to yield a training accuracy of 100\% and a validation accuracy of 75.9\%. 


\textbf{\textit{Inverse pyramid-shaped:}}
The inverse pyramid-shaped architecture is actually impossible to create because while you can maintain the original width and height, there is no way to increase it.

\subsection{Optimize your architecture}
The best performance we were able to achieve was a training accuracy of 90\% and a validation accuracy of 78.2\%.  This was achieved through using the original data set, setting the layer 1 filter size to 5, the layer 1 depth to 64, the layer 1 stride to 2, the layer 2 filter size to 10, the layer 2 depth to 16, and the layer 2 stride to 2. Furthermore, the layer 1 pool filter is 5, the layer 1 pool stride is 2, the layer 2 pool filter is 2, and the layer 2 pool stride is 2. Finally, we also had a weight penalty of 0.01 and a maximum training steps of 1500. 

\subsection{Test your final architecture on variations of the data}
With the optimal architecture that we found in the previous section, we tested our convolutional network on the transformed datasets. The following are the results we obtained for each of the transformations:

\begin{table}
  \begin{center}
    \begin{tabular}{ | c | c |}
      \hline
      Image Distortion & Validation Accuracy \\ \hline
      Translated   & 44.8          \\ \hline
      Brightened  & 37.9        \\ \hline
      Darkened & 69.0     \\ \hline
      High Contrast   & 37.9       \\ \hline
      Low Contrast   & 69.0       \\ \hline
      Flipped   & 51.7       \\ \hline
      Inverted   & 8.0       \\ \hline
    \end{tabular}
  \end{center}
  \caption{Performance on distorted data}
\end{table}


From these results there are two particular ones that surprised me. First, I was pretty surprised by the huge accuracy discrepancy between brightened data and darkened data. Another result that really intrigued me was seeing that inverted data only contained a 8\% accuracy. While I understand how the network would have problem with geometrical transformations, 8\% is literally how well a random algorithm would have performed. The transformations that my network is most invariant to are darken and low-contrast transformations. The transformations that lead to my network performing poorly are brightened data, high contrast data, and inverted data. What this means is my network has prioritized features that relate to the hue of the pixels. Specifically, the features probably depend on the overall magnitude of the hue as well as the relative hues of neighboring pixels. This explains why darkened and low contrast was able to perform well while brightened, high contrast, and inverted not so much. Additionally, it seemed like the network also learned some features related to the relative local positions of the pixels which is why translated and flipped were also able to do decently. 


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz,
% slightly modified from the 2009 version by Kiri Wagstaff and
% Sam Roweis's 2008 version, which is slightly modified from
% Prasad Tadepalli's 2007 version which is a lightly
% changed version of the previous year's version by Andrew Moore,
% which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
